{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f52ca170",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**1. What is the estimated depth of a Decision Tree trained (unrestricted) on a one million instance training set?**\n",
    "\n",
    "The estimated depth of a Decision Tree trained on a one million instance training set without any restrictions or pruning can be quite large. It is dependent on several factors, including the complexity of the data, the number and type of features, and the desired level of accuracy. In practice, the depth of a Decision Tree can range from a few levels to dozens or even hundreds of levels. It is not uncommon for large datasets to result in deep Decision Trees.\n",
    "\n",
    "**2. Is the Gini impurity of a node usually lower or higher than that of its parent? Is it always lower/greater, or is it usually lower/greater?**\n",
    "\n",
    "The Gini impurity of a node is usually lower than or equal to that of its parent. This is because the Gini impurity is a measure of the node's impurity or the level of class mixing within the node. When splitting a node into child nodes, the goal is to decrease the impurity and create more homogeneous subsets.\n",
    "\n",
    "However, there can be situations where the Gini impurity of a child node is higher than that of its parent, especially when the split results in a smaller child node with fewer instances. In such cases, the increase in impurity can occur due to sampling variations or random fluctuations. But overall, the general expectation is that the Gini impurity decreases or remains the same when splitting a node.\n",
    "\n",
    "**3. Explain if it's a good idea to reduce max depth if a Decision Tree is overfitting the training set?**\n",
    "\n",
    "If a Decision Tree is overfitting the training set (i.e., it performs well on the training data but poorly on unseen data), reducing the maximum depth can be a good idea. Overfitting occurs when the tree becomes too complex and captures noise or irrelevant patterns in the training data.\n",
    "\n",
    "By reducing the maximum depth, you limit the tree's complexity and prevent it from memorizing the training data too closely. This helps the tree generalize better to unseen instances. Reducing the maximum depth effectively prunes the tree and removes unnecessary splits, leading to a simpler model that is less prone to overfitting.\n",
    "\n",
    "However, it's essential to find the right balance in setting the maximum depth. If the depth is set too low, the model may become too simple and underfit the training data, resulting in decreased performance. It is crucial to evaluate the model's performance on a separate validation set or use cross-validation to find the optimal maximum depth for the specific problem.\n",
    "\n",
    "**4. Explain if it's a good idea to try scaling the input features if a Decision Tree underfits the training set?**\n",
    "\n",
    "Scaling the input features is typically not necessary for Decision Trees since they are not sensitive to monotonic transformations of the features. Decision Trees make splits based on the values of individual features and their thresholds, rather than the absolute scale or distribution of the features.\n",
    "\n",
    "If a Decision Tree underfits the training set (i.e., it has poor performance and struggles to capture the underlying patterns), the issue is unlikely to be related to feature scaling. It is more likely that other factors, such as insufficient depth, limited complexity, or inadequate data representation, are causing the underfitting.\n",
    "\n",
    "In such cases, it would be more appropriate to focus on adjusting the hyperparameters of the Decision Tree, such as the maximum depth, minimum samples for splitting, or the choice of the splitting criterion. Additionally, reviewing the feature selection or engineering process and ensuring that relevant features are included and noisy or irrelevant features are removed can also help improve the model's performance.\n",
    "\n",
    "**5. How much time will it take to train another Decision Tree on a training set of 10 million instances if it takes an hour to train a Decision Tree on a training set with 1 million instances?**\n",
    "\n",
    "The time required to train a Decision Tree is not directly proportional to the number of instances but depends on various factors such as the complexity of the data, the number and type of features, the selected hyperparameters, and the computational resources available.\n",
    "\n",
    "Assuming the training time for a Decision Tree on a 1 million instance dataset is one hour, we can estimate the time for a 10 million instance dataset based on the assumption that training time scales linearly with the number of instances. Using this assumption, training a Decision Tree on a 10 million instance dataset would take approximately 10 hours.\n",
    "\n",
    "However, it's important to note that this is a rough estimation and the actual training time can vary based on the factors mentioned earlier and the specific implementation and hardware used for training.\n",
    "\n",
    "**6. Will setting `presort=True` speed up training if your training set has 100,000 instances?**\n",
    "\n",
    "Setting `presort=True` in a Decision Tree classifier can potentially speed up training for small datasets, but it comes at the cost of increased memory consumption. The `presort` option enables pre-sorting of the data based on feature values, which can reduce the number of operations needed during tree construction.\n",
    "\n",
    "However, the benefit of using `presort=True` diminishes as the size of the dataset increases. For a training set with 100,000 instances, enabling `presort=True` may actually slow down the training process due to the overhead of sorting the larger dataset.\n",
    "\n",
    "In practice, it is recommended to experiment with different settings (e.g., comparing training time with and without `presort`) to determine the optimal approach based on the specific dataset and available computational resources.\n",
    "\n",
    "**7. Follow these steps to train and fine-tune a Decision Tree for the moons dataset:**\n",
    "\n",
    "a. To build a moons dataset, use `make_moons(n_samples=10000, noise=0.4)`.\n",
    "\n",
    "b. Divide the dataset into a training and a test collection with `train_test_split()`.\n",
    "\n",
    "c. To find good hyperparameter values for a `DecisionTreeClassifier`, use grid search with cross-validation (with the `GridSearchCV` class). Try different values for `max_leaf_nodes`.\n",
    "\n",
    "d. Use these hyperparameters to train the model on the entire training set, and then assess its output on the test set. You can achieve an accuracy of 85 to 87 percent.\n",
    "\n",
    "**8. Follow these steps to grow a forest:**\n",
    "\n",
    "a. Using the same method as before, create 1,000 subsets of the training set, each containing 100 instances chosen at random. You can do this with Scikit-Learn's `ShuffleSplit` class.\n",
    "\n",
    "b. Using the best hyperparameter values found in the previous exercise, train one Decision Tree on each subset. On the test collection, evaluate these 1,000 Decision Trees. These Decision Trees would likely perform worse than the first Decision Tree, achieving only around 80% accuracy, since they were trained on smaller sets.\n",
    "\n",
    "c. Now the magic begins. Create 1,000 Decision Tree predictions for each test set case, and keep only the most common prediction (you can do this with SciPy's `mode()` function). Over the test collection, this method gives you majority-vote predictions.\n",
    "\n",
    "d. On the test range, evaluate these predictions: you should achieve a slightly higher accuracy than the first model (approximately 0.5 to 1.5 percent higher). You've successfully learned a Random Forest classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f420ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
